{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5647809b",
   "metadata": {},
   "source": [
    "# 3. Treinando e Testando o Modelo\n",
    "\n",
    "Após o pré-processamento completo dos dados, o próximo passo é construir um modelo de classificação para prever o **sucesso das startups**.\n",
    "\n",
    "Inicialmente, testamos o **XGBClassifier**, que apresentou **ótimos resultados** em termos de acurácia e desempenho geral.  \n",
    "No entanto, como nosso objetivo era utilizar **apenas modelos da biblioteca `scikit-learn`**, não pudemos seguir com o XGBoost.\n",
    "\n",
    "Diante disso, optamos pelo **RandomForestClassifier**, que possui características semelhantes ao XGBClassifier:\n",
    "\n",
    "- Baseado em **árvores de decisão**, mas construindo múltiplas árvores para formar um **ensemble robusto**.\n",
    "- Capaz de lidar bem com **dados mistos** (numéricos e categóricos codificados) e **features com diferentes escalas**.\n",
    "- Robusto contra **overfitting** quando comparado a uma única árvore de decisão.\n",
    "- Muito eficiente para conjuntos de dados estruturados como o nosso, permitindo boa **generalização**.\n",
    "\n",
    "Dessa forma, o RandomForest se tornou o modelo ideal para continuar os experimentos e análises preditivas neste dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19205d",
   "metadata": {},
   "source": [
    "### 3.1 Importando o que usaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939ff7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2b23b",
   "metadata": {},
   "source": [
    "### 3.2 Carregando os Dados Processados\n",
    "\n",
    "Após o pré-processamento e feature engineering, salvamos os datasets tratados em arquivos CSV separados (`train_processado.csv` e `test_processado.csv`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3048cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_processado.csv')\n",
    "df_test = pd.read_csv('test_processado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc573cd6",
   "metadata": {},
   "source": [
    "### 3.3 Treinamento do Modelo: Random Forest\n",
    "\n",
    "Nesta etapa, realizamos o treinamento do modelo de classificação para prever a variável `labels`. Inicialmente, testamos o `XGBClassifier`, que teve ótimo desempenho, mas como ele **não pertence à biblioteca `sklearn`**, optamos por usar o `RandomForestClassifier`, que possui mecânica semelhante e também é muito eficaz para datasets estruturados como este.\n",
    "\n",
    "\n",
    "#### 3.3.1 Separação entre Features e Target\n",
    "\n",
    "Nesta etapa, definimos as features (`X`) e a variável alvo (`y`). Também removemos colunas irrelevantes, como `id` e `situacao_funding`, que não fornecem informação útil para o modelo.\n",
    "\n",
    "```python\n",
    "# Remover colunas irrelevantes ou auxiliares\n",
    "drop_cols = [col for col in ['id', 'situacao_funding'] if col in df_train.columns]\n",
    "\n",
    "# Features (X) e Target (y)\n",
    "X = df_train.drop(columns=['labels'] + drop_cols)\n",
    "y = df_train['labels']\n",
    "```\n",
    "#### 3.3.2. Divisão em treino e validação\n",
    "\n",
    "Aqui, dividimos os dados em conjuntos de treino e validação. Usamos 20% para validação e mantemos a proporção das classes com `stratify=y`.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,       # 20% dos dados para validação\n",
    "    random_state=42,     # garante reprodutibilidade\n",
    "    stratify=y           # mantém proporção das classes\n",
    ")\n",
    "```\n",
    "#### 3.3.3. Definição do Modelo Base e Espaço de Hiperparâmetros\n",
    "\n",
    "Definimos o modelo `RandomForestClassifier` base e o espaço de hiperparâmetros que será explorado no `RandomizedSearchCV`.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modelo base\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  # importante para lidar com desbalanceamento\n",
    ")\n",
    "\n",
    "# Espaço de hiperparâmetros\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 500, 800, 1000],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.3.4. Randomized Search para Otimização de Hiperparâmetros\n",
    "\n",
    "Nesta etapa, usamos o `RandomizedSearchCV` para explorar combinações aleatórias dos hiperparâmetros definidos, buscando a melhor configuração para o modelo Random Forest. Essa abordagem é mais rápida do que testar todas as combinações possíveis (Grid Search) e ainda consegue encontrar bons resultados.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,          # número de combinações aleatórias testadas\n",
    "    cv=3,               # validação cruzada com 3 folds\n",
    "    scoring='accuracy', # métrica utilizada para avaliar desempenho\n",
    "    n_jobs=-1,          # paraleliza o processo\n",
    "    verbose=2,          # exibe o progresso\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Exibir melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros encontrados:\", random_search.best_params_)\n",
    "\n",
    "# Selecionar o melhor modelo final\n",
    "best_rf = random_search.best_estimator_\n",
    "```\n",
    "#### 3.3.5. Avaliação do Modelo no Conjunto de Validação\n",
    "\n",
    "Após treinar o modelo com os melhores hiperparâmetros encontrados pelo `RandomizedSearchCV`, é hora de avaliar seu desempenho no conjunto de validação. Nesta etapa, verificamos métricas essenciais para entender como o modelo generaliza para dados que ele ainda não viu.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Previsões no conjunto de validação\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# 1. Acurácia\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Acurácia no conjunto de validação: {acc:.4f}\")\n",
    "\n",
    "# 2. Relatório detalhado por classe\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "# 3. Matriz de Confusão\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"Matriz de Confusão:\\n\", cm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f8fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=15, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=15, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=500; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=15, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   1.1s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   2.0s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   2.3s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   2.5s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.2s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time=   2.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=800; total time=   2.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=800; total time=   2.1s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=800; total time=   2.1s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   1.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   1.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   1.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   1.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   1.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   2.3s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   2.0s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   2.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   2.0s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   2.3s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   2.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   1.9s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=6, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.1s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   1.7s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.0s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=5, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=5, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.2s\n",
      "[CV] END max_depth=10, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.1s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=5, n_estimators=500; total time=   1.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time=   2.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=   2.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   2.9s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   3.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   3.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   4.7s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   3.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=   4.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time=   5.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   5.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   5.3s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   4.4s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   5.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   5.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   5.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=6, min_samples_split=15, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=6, min_samples_split=15, n_estimators=800; total time=   4.3s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=6, min_samples_split=15, n_estimators=800; total time=   4.4s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=6, min_samples_split=15, n_estimators=800; total time=   3.2s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=None, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   3.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=10, n_estimators=1000; total time=   4.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.7s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=   2.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.7s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   4.0s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=2, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=2, n_estimators=500; total time=   2.2s\n",
      "[CV] END max_depth=40, max_features=None, min_samples_leaf=6, min_samples_split=2, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   3.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   3.1s\n",
      "[CV] END max_depth=None, max_features=None, min_samples_leaf=2, min_samples_split=15, n_estimators=800; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   2.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time=   2.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   1.3s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   2.4s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   2.6s\n",
      "[CV] END max_depth=30, max_features=None, min_samples_leaf=4, min_samples_split=15, n_estimators=800; total time=   2.3s\n",
      "Melhores parâmetros encontrados: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Separar features e target\n",
    "drop_cols = [col for col in ['id', 'situacao_funding'] if col in df_train.columns]\n",
    "X = df_train.drop(columns=['labels'] + drop_cols)\n",
    "y = df_train['labels']\n",
    "\n",
    "# 2. Dividir treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# Modelo base\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  # importante para dataset desbalanceado\n",
    ")\n",
    "\n",
    "# Espaço de hiperparâmetros mais amplo\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 500, 800, 1000],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,              # número de combinações aleatórias testadas\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros encontrados:\", random_search.best_params_)\n",
    "\n",
    "# Melhor modelo\n",
    "best_rf = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ea811",
   "metadata": {},
   "source": [
    "## 3.4 Treinando o modelo com base nos melhores parâmetros passados.\n",
    "\n",
    "### 3.4.1. Separação entre Features e Target\n",
    "\n",
    "Primeiro, definimos as variáveis de entrada (`X`) e a variável alvo (`y`). Colunas que não trazem informação relevante, como `id` e `situacao_funding`, são removidas.\n",
    "\n",
    "```python\n",
    "drop_cols = [col for col in ['id', 'situacao_funding'] if col in df_train.columns]\n",
    "X = df_train.drop(columns=['labels'] + drop_cols)\n",
    "y = df_train['labels']\n",
    "```\n",
    "### 3.4.2. Divisão entre Treino e Validação\n",
    "\n",
    "Nesta etapa, separamos os dados em conjuntos de treino e validação. O conjunto de treino será utilizado para ajustar o modelo, enquanto o conjunto de validação servirá para avaliar a performance do modelo em dados que ele ainda não viu. \n",
    "\n",
    "Usamos o parâmetro `test_size=0.2` para destinar 20% dos dados para validação. O parâmetro `stratify=y` garante que a proporção das classes no conjunto de treino e validação se mantenha igual à do dataset original, evitando desbalanceamento.\n",
    "\n",
    "```python\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "```\n",
    "### 3.4.3. Instanciação do Modelo\n",
    "\n",
    "Nesta etapa, instanciamos o modelo `RandomForestClassifier` com os melhores hiperparâmetros encontrados na busca aleatória (RandomizedSearchCV).  \n",
    "\n",
    "- `n_estimators=200`: número de árvores na floresta.  \n",
    "- `max_depth=30`: profundidade máxima de cada árvore.  \n",
    "- `min_samples_split=2`: número mínimo de amostras necessárias para dividir um nó.  \n",
    "- `min_samples_leaf=1`: número mínimo de amostras em um nó folha.  \n",
    "- `max_features='sqrt'`: número máximo de features a serem consideradas para a melhor divisão.  \n",
    "- `class_weight='balanced'`: ajusta automaticamente pesos das classes para lidar com desbalanceamento.  \n",
    "- `random_state=42`: garante reprodutibilidade dos resultados.  \n",
    "- `n_jobs=-1`: utiliza todos os núcleos de CPU disponíveis para acelerar o treinamento.  \n",
    "\n",
    "```python\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "```\n",
    "\n",
    "### 3.4.4. Treinamento do Modelo\n",
    "\n",
    "Nesta etapa, treinamos o modelo `RandomForestClassifier` utilizando os dados de treino (`X_train` e `y_train`).  \n",
    "O modelo irá aprender padrões presentes nas features para prever a variável alvo `labels`.\n",
    "\n",
    "```python\n",
    "# Treinar o modelo\n",
    "best_rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 3.4.5. Avaliação do Modelo\n",
    "\n",
    "Predizemos os valores no conjunto de validação (`X_val`) e avaliamos a performance do modelo utilizando métricas importantes:\n",
    "\n",
    "- **Acurácia:** proporção de previsões corretas.\n",
    "- **Relatório de Classificação:** inclui precision, recall e F1-score por classe.\n",
    "- **Matriz de Confusão:** mostra acertos e erros detalhados por classe.\n",
    "\n",
    "```python\n",
    "# Avaliação no conjunto de validação\n",
    "y_pred = best_rf.predict(X_val)\n",
    "print(\"Acurácia:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Matriz de confusão:\\n\", confusion_matrix(y_val, y_pred))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462cde15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.7769230769230769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.59      0.65        46\n",
      "           1       0.80      0.88      0.84        84\n",
      "\n",
      "    accuracy                           0.78       130\n",
      "   macro avg       0.76      0.73      0.74       130\n",
      "weighted avg       0.77      0.78      0.77       130\n",
      "\n",
      "Matriz de confusão:\n",
      " [[27 19]\n",
      " [10 74]]\n"
     ]
    }
   ],
   "source": [
    "# Separar features e target\n",
    "drop_cols = [col for col in ['id', 'situacao_funding'] if col in df_train.columns]\n",
    "X = df_train.drop(columns=['labels'] + drop_cols)\n",
    "y = df_train['labels']\n",
    "\n",
    "# Dividir treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Instanciar o modelo com os melhores parâmetros\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  # importante para classes desbalanceadas\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação no conjunto de validação\n",
    "y_pred = best_rf.predict(X_val)\n",
    "print(\"Acurácia:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Matriz de confusão:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb175b07",
   "metadata": {},
   "source": [
    "### 3.5 Análise dos Resultados do Modelo\n",
    "\n",
    "Após treinar o Random Forest, avaliamos seu desempenho no conjunto de validação. Os resultados obtidos foram:\n",
    "\n",
    "- **Acurácia:** 77,7% das previsões estão corretas, o que indica um bom desempenho geral do modelo.\n",
    "- **Precision e Recall por Classe:**\n",
    "  - Classe `0` (startups que não atingiram o sucesso):\n",
    "    - Precision: 0.73 → Das previsões feitas como classe 0, 73% estavam corretas.\n",
    "    - Recall: 0.59 → Apenas 59% das startups realmente da classe 0 foram corretamente identificadas.\n",
    "    - F1-Score: 0.65 → Métrica balanceada entre precision e recall.\n",
    "  - Classe `1` (startups bem-sucedidas):\n",
    "    - Precision: 0.80 → Das previsões feitas como classe 1, 80% estavam corretas.\n",
    "    - Recall: 0.88 → 88% das startups realmente bem-sucedidas foram corretamente identificadas.\n",
    "    - F1-Score: 0.84 → Excelente equilíbrio entre precision e recall.\n",
    "\n",
    "- **Matriz de Confusão:**\n",
    "[[27 19]\n",
    "[10 74]]\n",
    "\n",
    "  - Linha 0: 27 startups da classe 0 foram corretamente classificadas, 19 foram classificadas incorretamente como classe 1.\n",
    "  - Linha 1: 74 startups da classe 1 foram corretamente classificadas, 10 foram classificadas incorretamente como classe 0.\n",
    "\n",
    "**Conclusão:**  \n",
    "O modelo apresenta desempenho melhor para a classe 1 (startups bem-sucedidas), conseguindo identificar a maioria corretamente. Para a classe 0, o recall é menor, indicando que algumas startups sem sucesso estão sendo classificadas como bem-sucedidas.  \n",
    "Ainda assim, considerando o desbalanceamento do dataset, o resultado geral é muito bom e consistente com expectativas para este tipo de problema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e930c8",
   "metadata": {},
   "source": [
    "## 3.6 Previsão e Geração do Arquivo de Submissão\n",
    "\n",
    "Nesta etapa, utilizamos o modelo Random Forest treinado para prever os rótulos do conjunto de teste e gerar o arquivo de submissão.\n",
    "\n",
    "#### 3.6.1 Preparação das Features do Conjunto de Teste\n",
    "\n",
    "Removemos colunas irrelevantes ou auxiliares (`id`, `situacao_funding` e `labels`, se existirem) para manter apenas as features que o modelo espera.\n",
    "\n",
    "```python\n",
    "drop_cols_test = [col for col in ['id', 'situacao_funding', 'labels'] if col in df_test.columns]\n",
    "X_test = df_test.drop(columns=drop_cols_test)\n",
    "```\n",
    "#### 3.6.2. Previsão dos Rótulos\n",
    "\n",
    "Nesta etapa, aplicamos o modelo Random Forest previamente treinado (`best_rf`) para gerar previsões sobre o conjunto de teste.  \n",
    "Ou seja, o modelo utiliza as features de entrada (`X_test`) para estimar a classe (`label`) de cada amostra.\n",
    "\n",
    "```python\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "```\n",
    "\n",
    "#### 3.6.3 Criação e Salvamento do Arquivo de Submissão\n",
    "\n",
    "Após gerar as previsões com o modelo, organizamos os resultados em um DataFrame contendo as colunas `id` e `labels` e salvamos em formato CSV, que pode ser enviado para submissão.\n",
    "\n",
    "```python\n",
    "# Criar DataFrame de submissão\n",
    "df_submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'labels': y_test_pred\n",
    "})\n",
    "\n",
    "# Salvar em CSV\n",
    "df_submission.to_csv('submission_rf_DECISIVO_CRISTIANO_ROMERO.csv', index=False)\n",
    "print(\"Arquivo de submissão salvo como submission_rf.csv\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0c3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de submissão salvo como submission_rf.csv\n"
     ]
    }
   ],
   "source": [
    "# Separar features do conjunto de teste\n",
    "drop_cols_test = [col for col in ['id', 'situacao_funding', 'labels'] if col in df_test.columns]\n",
    "X_test = df_test.drop(columns=drop_cols_test)\n",
    "\n",
    "# Prever os rótulos do conjunto de teste\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Salvar para submissão\n",
    "df_submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'labels': y_test_pred\n",
    "})\n",
    "df_submission.to_csv('submission_rf_DECISIVO_CRISTIANO_ROMERO.csv', index=False)\n",
    "print(\"Arquivo de submissão salvo como submission_rf.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
